{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punct_beginning = [\"༄\", \"༅\", \"࿓\", \"࿔\", \"༇\", \"༆\", \"༈\"]\n",
    "punct_separators = [\" \", \"།\", \"༎\", \"༏\", \"༐\", \"༑\", \"༔\"]\n",
    "punct_other = [\"༼\", \"༽\", \"༒\", \"༓\"]\n",
    "text_punct = punct_beginning+punct_separators+punct_other\n",
    "syl_punct = [\"་\", \"༌\", \"ཿ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class prepareTib:\n",
    "    ''' Dealing with the punctuation of Tibetan '''\n",
    "    \n",
    "    def __init__(self, tibstring):\n",
    "        global current_text\n",
    "        if current_text != []:\n",
    "            current_text = []\n",
    "        \n",
    "        self.raw = tibstring\n",
    "        \n",
    "        self.b_punct = punct_beginning        \n",
    "        self.t_punct = text_punct\n",
    "        \n",
    "        self.s_punct = syl_punct\n",
    "        \n",
    "        self.any_s_punct = '['+''.join(self.s_punct)+']+'\n",
    "        self.any_t_punct = '['+''.join(self.t_punct)+']+'\n",
    "        self.any_b_punct = '['+''.join(self.b_punct)+']+'\n",
    "        \n",
    "        #####################################################\n",
    "        # Prepare the text representation\n",
    "        p_splitted = re.split('('+''.join(self.any_t_punct)+')', self.raw)\n",
    "        p_splitted = [punct for punct in p_splitted if punct != ''] # eliminate empty elements\n",
    "        for elt in p_splitted:\n",
    "            # initialize the tuple (syl, {})\n",
    "            if re.findall(self.any_s_punct, elt):\n",
    "                syls = re.split(self.any_s_punct, elt)\n",
    "                syls = [syl for syl in syls if syl != ''] # eliminate empty elements\n",
    "                for syl in syls:\n",
    "                    current_text.append((syl, {}))\n",
    "            else:\n",
    "                if re.findall(self.any_b_punct, elt):\n",
    "                    current_text.append((elt, {'punct' : 1, 'pu_type' : 0}))\n",
    "                elif '༎'in elt:\n",
    "                    current_text.append((elt, {'punct' : 1, 'pu_type' : 2}))\n",
    "                elif re.findall(self.any_t_punct, elt):\n",
    "                    current_text.append((elt, {'punct' : 1, 'pu_type' : 1}))\n",
    "                else:\n",
    "                    current_text.append((elt, {}))\n",
    "        # tag the linebreaks\n",
    "        i = 0\n",
    "        while i <= len(current_text)-1:\n",
    "            for lb in ['\\n', '\\r\\n']:                \n",
    "                if lb in current_text[i][0]:\n",
    "                    elt = current_text[i][0].replace(lb, '')\n",
    "                    attribs = current_text[i][1]\n",
    "                    attribs['lbreak'] = 1\n",
    "                    current_text[i] = (elt, attribs)\n",
    "            i = i+1\n",
    "        #\n",
    "        ################################################################\n",
    "    \n",
    "    def get_all_punct(self):\n",
    "        global current_text\n",
    "        \n",
    "        out = []\n",
    "        for elt in current_text:\n",
    "            if 'punct' in elt[1].keys():\n",
    "                out.append(elt[0])\n",
    "            else:\n",
    "                out.append(elt[0]+'་')\n",
    "                \n",
    "        # extra tshek deletion\n",
    "        for num, syl in enumerate(out):\n",
    "            if syl.endswith('་') and syl[-2] != 'ང':\n",
    "                if out[num+1][0] in self.t_punct:\n",
    "                    out[num] = out[num][:-1]\n",
    "        return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('༄༅།། །།', {'pu_type': 0, 'punct': 1}), ('བཅོམ', {}), ('ལྡན', {}), ('འདས', {}), ('དེའི', {}), ('ཚེ', {}), ('ན', {}), ('ཚེ', {}), ('དང', {}), ('ལྡན', {}), ('པ', {}), ('ཀུན', {}), ('དགང', {}), ('། ', {'pu_type': 1, 'punct': 1}), ('ཀུན', {}), ('ཀུན', {}), ('༎', {'pu_type': 2, 'punct': 1})]\n"
     ]
    }
   ],
   "source": [
    "prepareTib(\"༄༅།། །།བཅོམ་ལྡན་འདས་དེའི་ཚེ་ན་ཚེ་དང་ལྡན་པ་ཀུན་དགང་། ཀུན་ཀུན༎\").get_all_punct()\n",
    "print(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_cases = [(\"འི\", 'dreldra'), (\"འང\", 'gyendu'), (\"འམ\", 'jedu'), (\"འོ\", 'lardu')]\n",
    "other = [(\"ས\", 'jedra'), (\"ར\", 'ladon')]\n",
    "\n",
    "dreldra = ['གི', 'ཀྱི', 'གྱི', 'ཡི']\n",
    "jedra = ['གིས', 'ཀྱིས', 'གྱིས', 'ཡིས']\n",
    "ladon = ['སུ', 'ཏུ', 'དུ', 'རུ']\n",
    "lhagche = ['སྟེ', 'ཏེ', 'དེ']\n",
    "gyendu = [\"ཀྱང\", \"ཡང\"]\n",
    "jedu = ['གམ', 'ངམ', 'དམ', 'ནམ', 'བམ', 'མམ', 'རམ', 'ལམ', 'སམ', 'ཏམ']\n",
    "dagdra = ['པ', 'པོ', 'བ', 'བོ']\n",
    "lardu = ['གོ', 'ངོ', 'དོ', 'ནོ', 'བོ', 'མོ', 'འོ', 'རོ', 'ལོ', 'སོ', 'ཏོ']\n",
    "separate_particles = dreldra + jedra + ladon + lhagche + gyendu + dagdra + lardu\n",
    "\n",
    "non_flex_particles = ['ནི', 'དང', 'ཅི', 'ཇི', 'སུ'] # find the particles that are often in composed lexicon entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (6, 8), (10, 13), (15, 15), (17, 17)]\n",
      "བཅོམ ལྡན འདས \n",
      "ཚེ ན ཚེ \n",
      "ལྡན པ ཀུན དགང \n",
      "ཀུན \n",
      "ཀུན \n",
      "[('༄༅།། །།', {'pu_type': 0, 'punct': 1}), ('བཅོམ', {}), ('ལྡན', {}), ('འདས', {}), ('ཀྱི', {'part': 1, 'pa_type': 'nonamb'}), ('དེའི', {'part': 1, 'lbreak': 1, 'pa_type': 'merged'}), ('ཚེ', {}), ('ན', {}), ('ཚེ', {'lbreak': 1}), ('དང', {'part': 1, 'pa_type': 'nonflex'}), ('ལྡན', {}), ('པ', {}), ('ཀུན', {'lbreak': 1}), ('དགང', {}), ('།', {'pu_type': 1, 'punct': 1}), ('ཀུན', {}), (' ', {'pu_type': 1, 'punct': 1}), ('ཀུན', {})]\n"
     ]
    }
   ],
   "source": [
    "class segment:\n",
    "    '''segment a given string of Tibetan'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.m_particles = [c[0] for c in merged_cases] # ས and ར are not yet supported\n",
    "        self.s_particles = separate_particles\n",
    "        self.n_particles = non_flex_particles\n",
    "        self.text = current_text\n",
    "        self.nonamb_part = ['གི', 'ཀྱི', 'གྱི', 'གིས', 'ཀྱིས', 'ཡིས', 'ཏུ', 'རུ', 'སྟེ', 'ཏེ', 'ཀྱང', 'ཡང', 'འང', 'མམ', 'འམ', 'སམ', 'ཏམ', 'ནོ', 'ཏོ', 'ཅིང', 'ཅེས', 'ཅེའོ', 'ཅིག', 'ཞེས', 'ཞེའོ', 'ཞིག', 'ཤིང', 'ཤེའོ', 'ཤིག']\n",
    "        self.amb_part = ['ཡི', 'གྱིས', 'སུ', 'དུ', 'ལམ', 'གམ', 'ངམ', 'དམ', 'ནམ', 'བམ', 'རམ', 'པ', 'པོ', 'བ', 'བོ', 'གོ', 'ངོ', 'དོ', 'མོ', 'འོ', 'རོ', 'ལོ', 'སོ', 'དེ',  'ཞིང']\n",
    "\n",
    "    def is_punct(elt):\n",
    "        if 'punct' in elt[1].keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def is_part(elt):\n",
    "        if 'part' in elt[1].keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def tag_particles(self):\n",
    "        global current_text\n",
    "        \n",
    "        for elt in current_text:\n",
    "            if not segment.is_punct(elt): # leave out all punctuation                 \n",
    "                # tag unfusioned flexional particles\n",
    "                if elt[0] in self.nonamb_part:\n",
    "                    elt[1]['part'] = 1\n",
    "                    elt[1]['pa_type'] = 'nonamb'\n",
    "                # tag all the non-flexional particles\n",
    "                elif elt[0] in self.n_particles:\n",
    "                    elt[1]['part'] = 1\n",
    "                    elt[1]['pa_type'] = 'nonflex'\n",
    "                # tag the merged particles\n",
    "                elif elt[0][-2:] in self.m_particles:\n",
    "                    elt[1]['part'] = 1\n",
    "                    elt[1]['pa_type'] = 'merged'\n",
    "    \n",
    "    def syl_clusters(self):\n",
    "        global current_text\n",
    "        \n",
    "        clusters = []\n",
    "        separators = 'punct|part'\n",
    "        beginning = 0\n",
    "        end = 0\n",
    "        index = 0\n",
    "        for index, elt in enumerate(current_text):\n",
    "            if index == 0:\n",
    "                    if segment.is_punct(elt) or segment.is_part(elt):\n",
    "                        beginning = index\n",
    "            else:\n",
    "                    if not (segment.is_punct(elt) or segment.is_part(elt)) and (segment.is_punct(current_text[index-1]) or segment.is_part(current_text[index-1])):\n",
    "                        beginning = index\n",
    "                    if (segment.is_punct(elt) or segment.is_part(elt)) and not (segment.is_punct(current_text[index-1]) or segment.is_part(current_text[index-1])):\n",
    "                        end = index-1\n",
    "                        clusters.append((beginning, end))\n",
    "                    if index == len(current_text)-1:# and not segment.is_punct(elt):\n",
    "                        end = index\n",
    "                        clusters.append((beginning, end))\n",
    "        \n",
    "        if (beginning, end) != clusters[-1]:\n",
    "            clusters.append((beginning, end))\n",
    "        print(clusters)\n",
    "        return clusters\n",
    "                        \n",
    "                             \n",
    "string = '''༄༅།། །།བཅོམ་ལྡན་འདས་ཀྱི་\n",
    "དེའི་ཚེ་ན་\n",
    "ཚེ་དང་ལྡན་པ་\n",
    "ཀུན་དགང་།ཀུན ་ཀུན་'''\n",
    "prepareTib(string)\n",
    "segment().tag_particles()\n",
    "for cluster in segment().syl_clusters():\n",
    "    index = cluster[0]\n",
    "    while index <= cluster[1]:\n",
    "        print(current_text[index][0], end = ' ')\n",
    "        index = index+1\n",
    "    print()\n",
    "print(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ཀུན', {})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_text[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b7f50ca005b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#tout avec les particules entre des +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msyl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'particles'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msyl\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msyl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'syllables'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'་'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'current_layers' is not defined"
     ]
    }
   ],
   "source": [
    "#tout avec les particules entre des +\n",
    "for num, syl in enumerate(current_layers['particles']):\n",
    "    if syl == '':\n",
    "        syl = current_layers['syllables'][num][0]\n",
    "        print(syl, end = '་')\n",
    "    else:\n",
    "        print(' +'+str(syl)+'་+ ', end = '')\n",
    "print()\n",
    "\n",
    "# juste les blocs de syllabes\n",
    "for num, syl in enumerate(current_layers['particles']):\n",
    "    if syl == '':\n",
    "        syl = current_layers['syllables'][num][0]\n",
    "        print(syl, end = '་')\n",
    "    else:\n",
    "        print(end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-814a0c1b0cc6>, line 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-814a0c1b0cc6>\"\u001b[1;36m, line \u001b[1;32m76\u001b[0m\n\u001b[1;33m    (/\"གི\", \"ཀྱི\", \"གྱི\", \"ཡི\", \"གིས\", \"ཀྱིས\", \"གྱིས\", \"ཡིས\", \"སུ\", \"ཏུ\", \"དུ\", \"རུ\", \"སྟེ\", \"ཏེ\", \"དེ\", \"གམ\", \"ངམ\", \"དམ\", \"ནམ\", \"བམ\", \"མམ\", \"རམ\", \"ལམ\", \"སམ\", \"ཏམ\", \"པ\", \"པོ\", \"བ\", \"བོ\", \"གོ\", \"ངོ\", \"དོ\", \"ནོ\", \"བོ\", \"མོ\", \"འོ\", \"རོ\", \"ལོ\", \"སོ\", \"ཏོ\")\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# \"separate-particles\" : [list-of-preceeding-suffix]\n",
    "{\n",
    "\"གི\" : [\"ག\", \"ང\"],\n",
    "\"ཀྱི\" : [\"ད\", \"བ\", \"ས\"],\n",
    "\"གྱི\" : [\"ན\", \"མ\", \"ར\", \"ལ\"],\n",
    "\"ཡི\" : [\"འ\", \"\"],\n",
    "\"གིས\" : [\"ག\", \"ང\"],\n",
    "\"ཀྱིས\" : [\"ད\", \"བ\", \"ས\"],\n",
    "\"གྱིས\" : [\"ན\", \"མ\", \"ར\", \"ལ\"],\n",
    "\"ཡིས\" : [\"འ\", \"\"],\n",
    "\"སུ\" : [\"ས\"],\n",
    "\"ཏུ\" : [\"ག\", \"བ\", \"ད་དྲག\"],\n",
    "\"དུ\" : [\"ང\", \"ད\", \"ན\", \"མ\", \"ར\", \"ལ\"],\n",
    "\"རུ\" : [\"འ\", \"\"],\n",
    "\"སྟེ\" : [\"ག\", \"ང\", \"བ\", \"མ\", \"འ\", \"\"],\n",
    "\"ཏེ\" : [\"ན\", \"ར\", \"ལ\", \"ས\"],\n",
    "\"དེ\" : [\"ད\"],\n",
    "\"ཀྱང\" : [\"ག\", \"ད\", \"བ\", \"ས\"],\n",
    "\"ཡང\" : [\"ང\", \"ན\", \"མ\", \"འ\", \"ར\", \"ལ\", \"\"],\n",
    "\"འང\" : [\"འ\", \"\"],\n",
    "\"གམ\" : [\"ག\"],\n",
    "\"ངམ\" : [\"ང\"],\n",
    "\"དམ\" : [\"ད\"],\n",
    "\"ནམ\" : [\"ན\"],\n",
    "\"བམ\" : [\"བ\"],\n",
    "\"མམ\" : [\"མ\"],\n",
    "\"འམ\" : [\"འ\"],\n",
    "\"རམ\" : [\"ར\"],\n",
    "\"ལམ\" : [\"ལ\"],\n",
    "\"སམ\" : [\"ས\"],\n",
    "\"ཏམ\" : [\"ད་དྲག\"],\n",
    "\"པ\" : [\"ག\", \"ད\", \"བ\", \"ས\", \"ན\", \"མ\"],\n",
    "\"པོ\" : [\"ག\", \"ད\", \"བ\", \"ས\", \"ན\", \"མ\"],\n",
    "\"བ\" : [\"ང\", \"འ\", \"ར\", \"ལ\", \"\"],\n",
    "\"བོ\" : [\"ང\", \"འ\", \"ར\", \"ལ\", \"\"],\n",
    "\"གོ\" : [\"ག\"],\n",
    "\"ངོ\" : [\"ང\"],\n",
    "\"དོ\" : [\"ད\"],\n",
    "\"ནོ\" : [\"ན\"],\n",
    "\"བོ\" : [\"བ\"],\n",
    "\"མོ\" : [\"མ\"],\n",
    "\"འོ\" : [\"འ\"],\n",
    "\"རོ\" : [\"ར\"],\n",
    "\"ལོ\" : [\"ལ\"],\n",
    "\"སོ\" : [\"ས\"],\n",
    "\"ཏོ\" : [\"ད་དྲག\"],\n",
    "\"ཅིང\" : [\"ག\", \"ད\", \"བ\", \"ད་དྲག\"],\n",
    "\"ཅེས\" : [\"ག\", \"ད\", \"བ\", \"ད་དྲག\"],\n",
    "\"ཅེའོ\" : [\"ག\", \"ད\", \"བ\", \"ད་དྲག\"],\n",
    "\"ཅེ་ན\" : [\"ག\", \"ད\", \"བ\", \"ད་དྲག\"],\n",
    "\"ཅིག\" : [\"ག\", \"ད\", \"བ\", \"ད་དྲག\"],\n",
    "\"ཞིང\" : [\"ང\", \"ན\", \"མ\", \"འ\", \"ར\", \"ལ\", \"\"],\n",
    "\"ཞེས\" : [\"ང\", \"ན\", \"མ\", \"འ\", \"ར\", \"ལ\", \"ས\", \"\"],\n",
    "\"ཞེའོ\" : [\"ང\", \"ན\", \"མ\", \"འ\", \"ར\", \"ལ\", \"\"],\n",
    "\"ཞེ་ན\" : [\"ང\", \"ན\", \"མ\", \"འ\", \"ར\", \"ལ\", \"\"],\n",
    "\"ཞིག\" : [\"ང\", \"ན\", \"མ\", \"འ\", \"ར\", \"ལ\", \"\"],\n",
    "\"ཤིང\" : [\"ས\"],\n",
    "\"ཤེའོ\" : [\"ས\"],\n",
    "\"ཤེ་ན\" : [\"ས\"],\n",
    "\"ཤིག\" : [\"ས\"]\n",
    "}\n",
    "\n",
    "//\"གི\" \"ཀྱི\" \"གྱི\" \"ཡི\" \"གིས\" \"ཀྱིས\" \"གྱིས\" \"ཡིས\" \"སུ\" \"ཏུ\" \"དུ\" \"རུ\" \"སྟེ\" \"ཏེ\" \"དེ\" \"གམ\" \"ངམ\" \"དམ\" \"ནམ\" \"བམ\" \"མམ\" \"རམ\" \"ལམ\" \"སམ\" \"ཏམ\" \"པ\" \"པོ\" \"བ\" \"བོ\" \"གོ\" \"ངོ\" \"དོ\" \"ནོ\" \"བོ\" \"མོ\" \"འོ\" \"རོ\" \"ལོ\" \"སོ\" \"ཏོ\" \n",
    "{\"གི\" : true, \"ཀྱི\" : true, \"གྱི\" : true, \"ཡི\" : true, \"གིས\" : true, \"ཀྱིས\" : true, \"གྱིས\" : true, \"ཡིས\" : true, \"སུ\" : true, \"ཏུ\" : true, \"དུ\" : true, \"རུ\" : true, \"སྟེ\" : true, \"ཏེ\" : true, \"དེ\" : true, \"གམ\" : true, \"ངམ\" : true, \"དམ\" : true, \"ནམ\" : true, \"བམ\" : true, \"མམ\" : true, \"རམ\" : true, \"ལམ\" : true, \"སམ\" : true, \"ཏམ\" : true, \"པ\" : true, \"པོ\" : true, \"བ\" : true, \"བོ\" : true, \"གོ\" : true, \"ངོ\" : true, \"དོ\" : true, \"ནོ\" : true, \"བོ\" : true, \"མོ\" : true, \"འོ\" : true, \"རོ\" : true, \"ལོ\" : true, \"སོ\" : true, \"ཏོ\" : true}\n",
    "\n",
    "//\"ཅི\" \"ཇི\" \"ཡིན\" \"མིན\" \"ཡོད\" \"མེད\" \"དང\" \"ལ\" \"ལས\" \"ན\" \"ནི\" \n",
    "//\"ཀྱང\" \"ཡང\" \"ཅིང\" \"ཅེས\" \"ཅེའོ\" \"ཅིག\" \"ཞིང\" \"ཞེས\" \"ཞེའོ\" \"ཞིག\" \"ཤིང\" \"ཤེའོ\" \"ཤིག\" \"གིན\" \"གྱིན\"\n",
    "{\"ཅི\" : true, \"ཇི\" : true, \"ཡིན\" : true, \"མིན\" : true, \"ཡོད\" : true, \"མེད\" : true, \"དང\" : true, \"ལ\" : true, \"ལས\" : true, \"ན\" : true, \"ནི\" : true, \"ཀྱང\" : true, \"ཡང\" : true, \"ཅིང\" : true, \"ཅེས\" : true, \"ཅེའོ\" : true, \"ཅིག\" : true, \"ཞིང\" : true, \"ཞེས\" : true, \"ཞེའོ\" : true, \"ཞིག\" : true, \"ཤིང\" : true, \"ཤེའོ\" : true, \"ཤིག\" : true, \"གིན\" : true, \"གྱིན\" : true, \n",
    "\n",
    "//\"ཤེ་ན\" \"ཞེ་ན\" \"ཅེ་ན\" \n",
    "{\"ཤེ་ན\" : true, \"ཞེ་ན\" : true, \"ཅེ་ན\" : true, \n",
    "\n",
    "ལྡན་"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns False or the length of the suffix it had to remove to find the string\n",
    "def lookupStr(str):\n",
    "    if len(str) == 0:\n",
    "        return False\n",
    "    if search(str, 'particles'):\n",
    "        return {'suffixLength': 0, 'type': 'particle', 'str': str}\n",
    "    if search(str, 'words'):\n",
    "        return {'suffixLength': 0, 'type': 'word', 'str': str}\n",
    "    suffixLength = getSuffixLength(str)\n",
    "    if suffixLength == 0:\n",
    "        return False\n",
    "    strNoSuffix = str[0: -suffixLength]\n",
    "    if not str:\n",
    "        return False\n",
    "    if search(strNoSuffix+'འ', 'words'):\n",
    "        return {'suffixLength': suffixLength, 'type': 'word', 'ashung': True, 'str': str}\n",
    "    if search(strNoSuffix, 'words'):\n",
    "        return {'suffixLength': suffixLength, 'type': 'word', 'str': str}\n",
    "    if search(strNoSuffix, 'particles'):\n",
    "        return {'suffixLength': suffixLength, 'type': 'particle', 'str': str}\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(entry, listName):   # can't use a to specify default for hi\n",
    "    global lists, lists_len\n",
    "    # using bisect here divides computation time by a huge amount\n",
    "    pos = bisect_left(lists[listName],entry,0,lists_len[listName]) # gives the index of a given element(entry) in lists[listName]\n",
    "    return (True if pos != lists_len[listName] and lists[listName][pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the length of the suffix (or 0)\n",
    "# see the content of suffixes{} and second_suffixes{}. the meaning is not the normal one\n",
    "def getSuffixLength(str):\n",
    "    global suffixes, second_suffixes\n",
    "    if str[-2:] in second_suffixes and len(str) > 3 and not str[-3] == '་':\n",
    "        return 0\n",
    "    if str[-2:] in suffixes:\n",
    "        return 2\n",
    "    if str[-1:] in suffixes:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lists = {}\n",
    "lists_len = {}\n",
    "\n",
    "def addList(listName, fileName):\n",
    "    global lists, lists_len\n",
    "    if not listName in lists:\n",
    "        lists[listName] = []\n",
    "    with open(fileName, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip().strip('་')\n",
    "            if word != '':\n",
    "                lists[listName].append(word)\n",
    "    lists[listName] = sorted(lists[listName])\n",
    "    lists_len[listName] = len(lists[listName])\n",
    "\n",
    "addList('particles', 'src/particles.txt')\n",
    "addList('words', 'src/verbs.txt')\n",
    "addList('words', 'src/TDC.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
