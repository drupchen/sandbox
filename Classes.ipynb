{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punct_beginning = [\"༄\", \"༅\", \"࿓\", \"࿔\", \"༇\", \"༆\", \"༈\"]\n",
    "punct_separators = [\" \", \"།\", \"༎\", \"༏\", \"༐\", \"༑\", \"༔\"]\n",
    "punct_other = [\"༼\", \"༽\", \"༒\", \"༓\"]\n",
    "text_punct = punct_beginning+punct_separators+punct_other\n",
    "syl_punct = [\"་\", \"༌\", \"ཿ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class prepareTib:\n",
    "    ''' Dealing with the punctuation of Tibetan '''\n",
    "    \n",
    "    def __init__(self, tibstring):\n",
    "        global current_text\n",
    "        if current_text != []:\n",
    "            current_text = []\n",
    "        \n",
    "        self.raw = tibstring\n",
    "        \n",
    "        self.b_punct = punct_beginning        \n",
    "        self.t_punct = text_punct\n",
    "        \n",
    "        self.s_punct = syl_punct\n",
    "        \n",
    "        self.any_s_punct = '['+''.join(self.s_punct)+']+'\n",
    "        self.any_t_punct = '['+''.join(self.t_punct)+']+'\n",
    "        self.any_b_punct = '['+''.join(self.b_punct)+']+'\n",
    "        \n",
    "        #####################################################\n",
    "        # Prepare the text representation\n",
    "        p_splitted = re.split('('+''.join(self.any_t_punct)+')', self.raw)\n",
    "        p_splitted = [punct for punct in p_splitted if punct != ''] # eliminate empty elements\n",
    "        for elt in p_splitted:\n",
    "            # initialize the tuple (syl, {})\n",
    "            if re.findall(self.any_s_punct, elt):\n",
    "                syls = re.split(self.any_s_punct, elt)\n",
    "                syls = [syl for syl in syls if syl != ''] # eliminate empty elements\n",
    "                for syl in syls:\n",
    "                    current_text.append((syl, {}))\n",
    "            else:\n",
    "                if re.findall(self.any_b_punct, elt):\n",
    "                    current_text.append((elt, {'punct' : 1, 'pu_type' : 0}))\n",
    "                elif '༎'in elt:\n",
    "                    current_text.append((elt, {'punct' : 1, 'pu_type' : 2}))\n",
    "                elif re.findall(self.any_t_punct, elt):\n",
    "                    current_text.append((elt, {'punct' : 1, 'pu_type' : 1}))\n",
    "                else:\n",
    "                    current_text.append((elt, {}))\n",
    "        # tag the linebreaks\n",
    "        i = 0\n",
    "        while i <= len(current_text)-1:\n",
    "            for lb in ['\\n', '\\r\\n']:                \n",
    "                if lb in current_text[i][0]:\n",
    "                    elt = current_text[i][0].replace(lb, '')\n",
    "                    attribs = current_text[i][1]\n",
    "                    attribs['lbreak'] = 1\n",
    "                    current_text[i] = (elt, attribs)\n",
    "            i = i+1\n",
    "        #\n",
    "        ################################################################\n",
    "    \n",
    "    def get_all_punct(self):\n",
    "        global current_text\n",
    "        \n",
    "        out = []\n",
    "        for elt in current_text:\n",
    "            if 'punct' in elt[1].keys():\n",
    "                out.append(elt[0])\n",
    "            else:\n",
    "                out.append(elt[0]+'་')\n",
    "                \n",
    "        # extra tshek deletion\n",
    "        for num, syl in enumerate(out):\n",
    "            if syl.endswith('་') and syl[-2] != 'ང':\n",
    "                if out[num+1][0] in self.t_punct:\n",
    "                    out[num] = out[num][:-1]\n",
    "        return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('༄༅།། །།', {'pu_type': 0, 'punct': 1}), ('བཅོམ', {}), ('ལྡན', {}), ('འདས', {}), ('དེའི', {}), ('ཚེ', {}), ('ན', {}), ('ཚེ', {}), ('དང', {}), ('ལྡན', {}), ('པ', {}), ('ཀུན', {}), ('དགང', {}), ('། ', {'pu_type': 1, 'punct': 1}), ('ཀུན', {}), ('ཀུན', {}), ('༎', {'pu_type': 2, 'punct': 1})]\n"
     ]
    }
   ],
   "source": [
    "prepareTib(\"༄༅།། །།བཅོམ་ལྡན་འདས་དེའི་ཚེ་ན་ཚེ་དང་ལྡན་པ་ཀུན་དགང་། ཀུན་ཀུན༎\").get_all_punct()\n",
    "print(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_cases = [(\"འི\", 'dreldra'), (\"འང\", 'gyendu'), (\"འམ\", 'jedu'), (\"འོ\", 'lardu')]\n",
    "other = [(\"ས\", 'jedra'), (\"ར\", 'ladon')]\n",
    "\n",
    "dreldra = ['གི', 'ཀྱི', 'གྱི', 'ཡི']\n",
    "jedra = ['གིས', 'ཀྱིས', 'གྱིས', 'ཡིས']\n",
    "ladon = ['སུ', 'ཏུ', 'དུ', 'རུ']\n",
    "lhagche = ['སྟེ', 'ཏེ', 'དེ']\n",
    "gyendu = [\"ཀྱང\", \"ཡང\"]\n",
    "jedu = ['གམ', 'ངམ', 'དམ', 'ནམ', 'བམ', 'མམ', 'རམ', 'ལམ', 'སམ', 'ཏམ']\n",
    "dagdra = ['པ', 'པོ', 'བ', 'བོ']\n",
    "lardu = ['གོ', 'ངོ', 'དོ', 'ནོ', 'བོ', 'མོ', 'འོ', 'རོ', 'ལོ', 'སོ', 'ཏོ']\n",
    "separate_particles = dreldra + jedra + ladon + lhagche + gyendu + dagdra + lardu\n",
    "\n",
    "non_flex_particles = ['ནི', 'དང', 'ཅི', 'ཇི', 'སུ'] # find the particles that are often in composed lexicon entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (6, 8), (10, 13), (15, 15), (17, 17)]\n",
      "བཅོམ ལྡན འདས \n",
      "ཚེ ན ཚེ \n",
      "ལྡན པ ཀུན དགང \n",
      "ཀུན \n",
      "ཀུན \n",
      "[('༄༅།། །།', {'pu_type': 0, 'punct': 1}), ('བཅོམ', {}), ('ལྡན', {}), ('འདས', {}), ('ཀྱི', {'part': 1, 'pa_type': 'nonamb'}), ('དེའི', {'part': 1, 'lbreak': 1, 'pa_type': 'merged'}), ('ཚེ', {}), ('ན', {}), ('ཚེ', {'lbreak': 1}), ('དང', {'part': 1, 'pa_type': 'nonflex'}), ('ལྡན', {}), ('པ', {}), ('ཀུན', {'lbreak': 1}), ('དགང', {}), ('།', {'pu_type': 1, 'punct': 1}), ('ཀུན', {}), (' ', {'pu_type': 1, 'punct': 1}), ('ཀུན', {})]\n"
     ]
    }
   ],
   "source": [
    "class segment:\n",
    "    '''segment a given string of Tibetan'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.m_particles = [c[0] for c in merged_cases] # ས and ར are not yet supported\n",
    "        self.s_particles = separate_particles\n",
    "        self.n_particles = non_flex_particles\n",
    "        self.text = current_text\n",
    "        self.nonamb_part = ['གི', 'ཀྱི', 'གྱི', 'གིས', 'ཀྱིས', 'ཡིས', 'ཏུ', 'རུ', 'སྟེ', 'ཏེ', 'ཀྱང', 'ཡང', 'འང', 'མམ', 'འམ', 'སམ', 'ཏམ', 'ནོ', 'ཏོ', 'ཅིང', 'ཅེས', 'ཅེའོ', 'ཅིག', 'ཞེས', 'ཞེའོ', 'ཞིག', 'ཤིང', 'ཤེའོ', 'ཤིག']\n",
    "        self.amb_part = ['ཡི', 'གྱིས', 'སུ', 'དུ', 'ལམ', 'གམ', 'ངམ', 'དམ', 'ནམ', 'བམ', 'རམ', 'པ', 'པོ', 'བ', 'བོ', 'གོ', 'ངོ', 'དོ', 'མོ', 'འོ', 'རོ', 'ལོ', 'སོ', 'དེ',  'ཞིང']\n",
    "\n",
    "    def is_punct(elt):\n",
    "        if 'punct' in elt[1].keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def is_part(elt):\n",
    "        if 'part' in elt[1].keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def tag_particles(self):\n",
    "        global current_text\n",
    "        \n",
    "        for elt in current_text:\n",
    "            if not segment.is_punct(elt): # leave out all punctuation                 \n",
    "                # tag unfusioned flexional particles\n",
    "                if elt[0] in self.nonamb_part:\n",
    "                    elt[1]['part'] = 1\n",
    "                    elt[1]['pa_type'] = 'nonamb'\n",
    "                # tag all the non-flexional particles\n",
    "                elif elt[0] in self.n_particles:\n",
    "                    elt[1]['part'] = 1\n",
    "                    elt[1]['pa_type'] = 'nonflex'\n",
    "                # tag the merged particles\n",
    "                elif elt[0][-2:] in self.m_particles:\n",
    "                    elt[1]['part'] = 1\n",
    "                    elt[1]['pa_type'] = 'merged'\n",
    "    \n",
    "    def syl_clusters(self):\n",
    "        global current_text\n",
    "        \n",
    "        clusters = []\n",
    "        separators = 'punct|part'\n",
    "        beginning = 0\n",
    "        end = 0\n",
    "        index = 0\n",
    "        for index, elt in enumerate(current_text):\n",
    "            if index == 0:\n",
    "                    if segment.is_punct(elt) or segment.is_part(elt):\n",
    "                        beginning = index\n",
    "            else:\n",
    "                    if not (segment.is_punct(elt) or segment.is_part(elt)) and (segment.is_punct(current_text[index-1]) or segment.is_part(current_text[index-1])):\n",
    "                        beginning = index\n",
    "                    if (segment.is_punct(elt) or segment.is_part(elt)) and not (segment.is_punct(current_text[index-1]) or segment.is_part(current_text[index-1])):\n",
    "                        end = index-1\n",
    "                        clusters.append((beginning, end))\n",
    "                    if index == len(current_text)-1:# and not segment.is_punct(elt):\n",
    "                        end = index\n",
    "                        clusters.append((beginning, end))\n",
    "        \n",
    "        if (beginning, end) != clusters[-1]:\n",
    "            clusters.append((beginning, end))\n",
    "        print(clusters)\n",
    "        return clusters\n",
    "                        \n",
    "                             \n",
    "string = '''༄༅།། །།བཅོམ་ལྡན་འདས་ཀྱི་\n",
    "དེའི་ཚེ་ན་\n",
    "ཚེ་དང་ལྡན་པ་\n",
    "ཀུན་དགང་།ཀུན ་ཀུན་'''\n",
    "prepareTib(string)\n",
    "segment().tag_particles()\n",
    "for cluster in segment().syl_clusters():\n",
    "    index = cluster[0]\n",
    "    while index <= cluster[1]:\n",
    "        print(current_text[index][0], end = ' ')\n",
    "        index = index+1\n",
    "    print()\n",
    "print(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ཀུན', {})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_text[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_layers' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b7f50ca005b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#tout avec les particules entre des +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msyl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'particles'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msyl\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msyl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'syllables'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'་'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'current_layers' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#tout avec les particules entre des +\n",
    "for num, syl in enumerate(current_layers['particles']):\n",
    "    if syl == '':\n",
    "        syl = current_layers['syllables'][num][0]\n",
    "        print(syl, end = '་')\n",
    "    else:\n",
    "        print(' +'+str(syl)+'་+ ', end = '')\n",
    "print()\n",
    "\n",
    "# juste les blocs de syllabes\n",
    "for num, syl in enumerate(current_layers['particles']):\n",
    "    if syl == '':\n",
    "        syl = current_layers['syllables'][num][0]\n",
    "        print(syl, end = '་')\n",
    "    else:\n",
    "        print(end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns False or the length of the suffix it had to remove to find the string\n",
    "def lookupStr(str):\n",
    "    if len(str) == 0:\n",
    "        return False\n",
    "    if search(str, 'particles'):\n",
    "        return {'suffixLength': 0, 'type': 'particle', 'str': str}\n",
    "    if search(str, 'words'):\n",
    "        return {'suffixLength': 0, 'type': 'word', 'str': str}\n",
    "    suffixLength = getSuffixLength(str)\n",
    "    if suffixLength == 0:\n",
    "        return False\n",
    "    strNoSuffix = str[0: -suffixLength]\n",
    "    if not str:\n",
    "        return False\n",
    "    if search(strNoSuffix+'འ', 'words'):\n",
    "        return {'suffixLength': suffixLength, 'type': 'word', 'ashung': True, 'str': str}\n",
    "    if search(strNoSuffix, 'words'):\n",
    "        return {'suffixLength': suffixLength, 'type': 'word', 'str': str}\n",
    "    if search(strNoSuffix, 'particles'):\n",
    "        return {'suffixLength': suffixLength, 'type': 'particle', 'str': str}\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(entry, listName):   # can't use a to specify default for hi\n",
    "    global lists, lists_len\n",
    "    # using bisect here divides computation time by a huge amount\n",
    "    pos = bisect_left(lists[listName],entry,0,lists_len[listName]) # gives the index of a given element(entry) in lists[listName]\n",
    "    return (True if pos != lists_len[listName] and lists[listName][pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the length of the suffix (or 0)\n",
    "# see the content of suffixes{} and second_suffixes{}. the meaning is not the normal one\n",
    "def getSuffixLength(str):\n",
    "    global suffixes, second_suffixes\n",
    "    if str[-2:] in second_suffixes and len(str) > 3 and not str[-3] == '་':\n",
    "        return 0\n",
    "    if str[-2:] in suffixes:\n",
    "        return 2\n",
    "    if str[-1:] in suffixes:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lists = {}\n",
    "lists_len = {}\n",
    "\n",
    "def addList(listName, fileName):\n",
    "    global lists, lists_len\n",
    "    if not listName in lists:\n",
    "        lists[listName] = []\n",
    "    with open(fileName, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip().strip('་')\n",
    "            if word != '':\n",
    "                lists[listName].append(word)\n",
    "    lists[listName] = sorted(lists[listName])\n",
    "    lists_len[listName] = len(lists[listName])\n",
    "\n",
    "addList('particles', 'src/particles.txt')\n",
    "addList('words', 'src/verbs.txt')\n",
    "addList('words', 'src/TDC.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}