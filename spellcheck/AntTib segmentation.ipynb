{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'Classes'",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0730fa9eadcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbisect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbisect_left\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mClasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'Classes'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from bisect import bisect_left\n",
    "import time\n",
    "import Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tsikchen.txt'",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b30f2c34cb40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import the lexicon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tsikchen.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf-8-sig'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# add all the particles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tsikchen.txt'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# import the lexicon\n",
    "one = time.time()\n",
    "with open('tsikchen.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    lexicon = [line.strip() for line in f.readlines()]\n",
    "# add all the particles\n",
    "lexicon.extend(['གི', 'ཀྱི', 'གྱི', 'ཡི', 'གིས', 'ཀྱིས', 'གྱིས', 'ཡིས', 'སུ', 'ཏུ', 'དུ', 'རུ', 'སྟེ', 'ཏེ', 'དེ', 'ཀྱང', 'ཡང', 'འང', 'གམ', 'ངམ', 'དམ', 'ནམ', 'བམ', 'མམ', 'འམ', 'རམ', 'ལམ', 'སམ', 'ཏམ', 'གོ', 'ངོ', 'དོ', 'ནོ', 'མོ', 'འོ', 'རོ', 'ལོ', 'སོ', 'ཏོ', 'ཅིང', 'ཅེས', 'ཅེའོ', 'ཅེ་ན', 'ཅིག', 'ཞིང', 'ཞེས', 'ཞེའོ', 'ཞེ་ན', 'ཞིག', 'ཤིང', 'ཤེའོ', 'ཤེ་ན', 'ཤིག', 'ལ', 'ན', 'ནས', 'ལས', 'ནི', 'དང', 'གང', 'ཅི', 'ཇི', 'གིན', 'གྱིན', 'ཀྱིན', 'ཡིན', 'པ', 'བ', 'པོ', 'བོ'])\n",
    "# add all Monlam verbs\n",
    "with open('monlam1_verbs.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    monlam_verbs = [line.strip() for line in f.readlines()]\n",
    "for entry in monlam_verbs:\n",
    "    verb = entry.split(' | ')[0]\n",
    "    if verb not in lexicon:\n",
    "        lexicon.append(verb)\n",
    "lexicon = sorted(lexicon)\n",
    "len_lexicon = len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the Monlam POS tags\n",
    "with open('./monlam1_pos.txt', 'r', -1, 'utf-8-sig') as f: # Monlam\n",
    "    monlam = [line.strip() for line in f.readlines()]\n",
    "monlam_pos = {}\n",
    "for line in monlam:\n",
    "    parts = line.split('—') # Monlam\n",
    "    monlam_pos[parts[0]] = parts[1] \n",
    "\n",
    "# import the Hill POS tags\n",
    "with open('hill_pos.txt', 'r', -1, 'utf-8-sig') as f: # Hill\n",
    "    hill = [line.strip() for line in f.readlines()]\n",
    "hill_pos = {}\n",
    "for line in hill:\n",
    "    parts = line.split('—') # Hill\n",
    "    hill_pos[parts[0]] = parts[1]\n",
    "two = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def get_pos(dict, key):\n",
    "    if key.endswith('་'):\n",
    "        key = key[:-1]\n",
    "    return dict.get(key)\n",
    "\n",
    "def get_main_pos(key, num):\n",
    "    if num == 0:\n",
    "        Dict = monlam_pos\n",
    "    if num == 1:\n",
    "        Dict = hill_pos\n",
    "    pos = get_pos(Dict, key)\n",
    "\n",
    "    main_pos = []\n",
    "    if pos != None:\n",
    "        # Monlam\n",
    "        if num == 0:\n",
    "            parts = pos.split('/')\n",
    "            for part in parts:\n",
    "                if part != '':\n",
    "                    main_pos.append(part.split(':')[0])\n",
    "\n",
    "        # Hill\n",
    "        if num == 1:\n",
    "            parts = pos.split('/')\n",
    "            for part in parts:\n",
    "                if part != '':\n",
    "                    if '.' in part:\n",
    "                        main_pos.append(part.split('.')[0])\n",
    "                    else:\n",
    "                        main_pos.append(part)        \n",
    "    return main_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(List, entry):\n",
    "    global len_lexicon\n",
    "    index = bisect_left(List, entry, 0, len_lexicon)\n",
    "    return(True if index != len_lexicon and List[index] == entry else False)\n",
    "    \n",
    "def isWord(maybe):\n",
    "    final = False\n",
    "    if search(lexicon, maybe) == True:\n",
    "        final = True\n",
    "    elif search(lexicon, re.sub(merged_part, '', maybe)) == True:\n",
    "        final = True\n",
    "    else:\n",
    "        last_syl = ''\n",
    "        if '་' in maybe:\n",
    "            last_syl = maybe.split('་')[-1]\n",
    "            if SylComponents().get_info(last_syl) == 'thame':\n",
    "                maybe = re.sub(merged_part, '', maybe) + 'འ'\n",
    "        else:\n",
    "            if SylComponents().get_info(maybe) == 'thame':\n",
    "                maybe = re.sub(merged_part, '', maybe) + 'འ'\n",
    "        if search(lexicon, maybe) == True:\n",
    "            final = True\n",
    "    return final\n",
    "\n",
    "def process(list1, list2, num):\n",
    "    global c\n",
    "    word = '་'.join(list1[c:c+num])\n",
    "    if search(lexicon, word) == False:\n",
    "        maybe = re.split(merged_part, word)\n",
    "        if search(lexicon, maybe[0]) == False and search(lexicon, maybe[0]+'འ'):\n",
    "            list2.append(maybe[0]+'འ')\n",
    "        else:\n",
    "            list2.append(maybe[0])\n",
    "        list2.append(maybe[1]+'་')\n",
    "        #del list1[:num]\n",
    "        c = c + num\n",
    "    else:\n",
    "        list2.append(word+\"་\")\n",
    "        #del list1[:num]\n",
    "        c = c + num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mark_unknown = 0\n",
    "mark = '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for file in os.listdir('./IN/'):\n",
    "    if file.endswith(\".txt\"):\n",
    "    #todo - replace \\n by \\s try: with open('drugs') as temp_file: \\n drugs = [line.rstrip('\\n') for line in temp_file]\n",
    "        try:\n",
    "            with open('./IN/' + file, 'r', -1, 'utf-8-sig') as f:\n",
    "                current_file = f.read().replace('\\n', '').replace('\\r\\n', '').replace('༌', '་')\n",
    "\n",
    "        except:\n",
    "            print(\"Save all IN files as UTF-8 and try again.\")\n",
    "            #input()\n",
    "    else:\n",
    "        print(\"\\nSave all IN files as text files and try again.\")\n",
    "        #input()\n",
    "    #start = time.time()\n",
    "    ######################\n",
    "    # Segmentation process\n",
    "\n",
    "    merged_part = r'(ར|ས|འི|འམ|འང|འོ)$'\n",
    "    punct_regex = r'([༄༅༆༇༈།༎༏༐༑༔\\s]+)'\n",
    "    \n",
    "    paragraphs = re.split(punct_regex, current_file)\n",
    "    if paragraphs[0] == '':\n",
    "        del paragraphs[0]\n",
    "    if paragraphs[len(paragraphs)-1] == '':\n",
    "        del paragraphs[len(paragraphs)-1]\n",
    "        \n",
    "    text = []\n",
    "    for par in paragraphs:\n",
    "        words = []\n",
    "        if re.match(punct_regex, par) == None:\n",
    "            syls = par.split('་')\n",
    "            if syls[0] == '':\n",
    "                del syls[0]\n",
    "            if syls[len(syls)-1] == '':\n",
    "                del syls[len(syls)-1]\n",
    "            #print('\\t', syls)\n",
    "            c = 0\n",
    "            while c < len(syls):\n",
    "                #if   isWord(syls[c:c+4]): process(syls, words, 4)\n",
    "                if len(syls[c:c+3]) == 3 and isWord('་'.join(syls[c:c+3])): \n",
    "                    #print('3', syls[c:c+3])\n",
    "                    process(syls, words, 3)\n",
    "                elif len(syls[c:c+2]) == 2 and isWord('་'.join(syls[c:c+2])): \n",
    "                    #print('2', syls[c:c+2])\n",
    "                    process(syls, words, 2)\n",
    "                elif len(syls[c:c+1]) == 1 and isWord('་'.join(syls[c:c+1])): \n",
    "                    #print('1', syls[c:c+1])\n",
    "                    process(syls, words, 1)\n",
    "                else:\n",
    "                    if mark_unknown == 0:\n",
    "                        words.append('་'.join(syls[c:c+1])+'་')\n",
    "                    elif mark_unknown == 1:\n",
    "                        words.append(mark+'་'.join(syls[c:c+1])+'་')\n",
    "                        print(syls[c:c+1])\n",
    "                    c = c + 1\n",
    "            paragraph = ' '.join(words)\n",
    "            if not paragraph.endswith('ང་'):\n",
    "                paragraph = paragraph[:-1]\n",
    "            #########\n",
    "            # add spaces at all tseks\n",
    "            paragraph = re.sub(r'་([^ ])', r'་ \\1', paragraph)\n",
    "            #\n",
    "            #########\n",
    "            text.append(paragraph)\n",
    "        else:\n",
    "            text.append(par)\n",
    "    #\n",
    "    ######################\n",
    "    text = ''.join(text)\n",
    "    \n",
    "    ######################\n",
    "    # Transpose to AntTib\n",
    "    text = AntTib().to_pw_text(text)\n",
    "    #\n",
    "    ######################\n",
    "    \n",
    "    # write output\n",
    "    with open('./OUT/' + 'anttib_' + file, 'w', -1, 'utf-8-sig') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(AntTib().to_pw_text('མདུན'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}