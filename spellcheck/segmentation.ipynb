{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from bisect import bisect_left\n",
    "import time\n",
    "w_nb = 50 # number of words before linebreaks in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the lexicon\n",
    "one = time.time()\n",
    "with open('tsikchen.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    lexicon = [line.strip() for line in f.readlines()]\n",
    "# add all the particles\n",
    "lexicon.extend(['གི', 'ཀྱི', 'གྱི', 'ཡི', 'གིས', 'ཀྱིས', 'གྱིས', 'ཡིས', 'སུ', 'ཏུ', 'དུ', 'རུ', 'སྟེ', 'ཏེ', 'དེ', 'ཀྱང', 'ཡང', 'འང', 'གམ', 'ངམ', 'དམ', 'ནམ', 'བམ', 'མམ', 'འམ', 'རམ', 'ལམ', 'སམ', 'ཏམ', 'གོ', 'ངོ', 'དོ', 'ནོ', 'མོ', 'འོ', 'རོ', 'ལོ', 'སོ', 'ཏོ', 'ཅིང', 'ཅེས', 'ཅེའོ', 'ཅེ་ན', 'ཅིག', 'ཞིང', 'ཞེས', 'ཞེའོ', 'ཞེ་ན', 'ཞིག', 'ཤིང', 'ཤེའོ', 'ཤེ་ན', 'ཤིག', 'ལ', 'ན', 'ནས', 'ལས', 'ནི', 'དང', 'གང', 'ཅི', 'ཇི', 'གིན', 'གྱིན', 'ཀྱིན', 'ཡིན', 'པ', 'བ', 'པོ', 'བོ'])\n",
    "# add all Monlam verbs\n",
    "with open('monlam1_verbs.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    monlam_verbs = [line.strip() for line in f.readlines()]\n",
    "for entry in monlam_verbs:\n",
    "    verb = entry.split(' | ')[0]\n",
    "    if verb not in lexicon:\n",
    "        lexicon.append(verb)\n",
    "lexicon = sorted(lexicon)\n",
    "len_lexicon = len(lexicon)\n",
    "        \n",
    "# import the Monlam POS tags\n",
    "with open('./monlam1_pos.txt', 'r', -1, 'utf-8-sig') as f: # Monlam\n",
    "    monlam = [line.strip() for line in f.readlines()]\n",
    "monlam_pos = {}\n",
    "for line in monlam:\n",
    "    parts = line.split('—') # Monlam\n",
    "    monlam_pos[parts[0]] = parts[1] \n",
    "\n",
    "# import the Hill POS tags\n",
    "with open('hill_pos.txt', 'r', -1, 'utf-8-sig') as f: # Hill\n",
    "    hill = [line.strip() for line in f.readlines()]\n",
    "hill_pos = {}\n",
    "for line in hill:\n",
    "    parts = line.split('—') # Hill\n",
    "    hill_pos[parts[0]] = parts[1]\n",
    "two = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def part_agreement(previous, particle):\n",
    "    final = previous[-1]\n",
    "    # added the ད་དྲག་ for all and the མཐའ་མེད་ for all in provision of all cases where an extra syllable is neede in verses\n",
    "    dreldra = {'ད' : 'ཀྱི', 'བ' : 'ཀྱི', 'ས' : 'ཀྱི', 'ག' : 'གི', 'ང' : 'གི', 'ན' : 'གྱི', 'མ' : 'གྱི', 'ར' : 'གྱི', 'ལ' : 'གྱི', 'འ' : 'ཡི', 'མཐའ་མེད' : 'ཡི', 'ད་དྲག' : 'གྱི'}\n",
    "    jedra = {'ད' : 'ཀྱིས', 'བ' : 'ཀྱིས', 'ས' : 'ཀྱིས', 'ག' : 'གིས', 'ང' : 'གིས', 'ན' : 'གྱིས', 'མ' : 'གྱིས', 'ར' : 'གྱིས', 'ལ' : 'གྱིས', 'འ' : 'ཡིས', 'མཐའ་མེད' : 'ཡིས', 'ད་དྲག' : 'གྱིས'}\n",
    "    ladon = {'ག' : 'ཏུ', 'བ' : 'ཏུ', 'ང' : 'དུ', 'ད' : 'དུ', 'ན' : 'དུ', 'མ' : 'དུ', 'ར' : 'དུ', 'ལ' : 'དུ', 'འ' : 'རུ', 'ས' : 'སུ', 'མཐའ་མེད' : 'རུ', 'ད་དྲག' : 'ཏུ'}\n",
    "    lhakce = {'ན' : 'ཏེ', 'ར' : 'ཏེ', 'ལ' : 'ཏེ', 'ས' : 'ཏེ', 'ད' : 'དེ', 'ག' : 'སྟེ', 'ང' : 'སྟེ', 'བ' : 'སྟེ', 'མ' : 'སྟེ', 'འ' : 'སྟེ', 'མཐའ་མེད' : 'སྟེ', 'ད་དྲག' : 'ཏེ'}\n",
    "    gyendu = {'ག' : 'ཀྱང', 'ད' : 'ཀྱང', 'བ' : 'ཀྱང', 'ས' : 'ཀྱང', 'འ' : 'འང', 'ང' : 'ཡང', 'ན' : 'ཡང', 'མ' : 'ཡང', 'འ' : 'ཡང', 'ར' : 'ཡང', 'ལ' : 'ཡང', 'མཐའ་མེད' : 'ཡང', 'ད་དྲག' : 'ཀྱང'}\n",
    "    jedu = {'ག' : 'གམ', 'ང' : 'ངམ', 'ད་དྲག' : 'ཏམ', 'ད' : 'དམ', 'ན' : 'ནམ', 'བ' : 'བམ', 'མ' : 'མམ', 'འ' : 'འམ', 'ར' : 'རམ', 'ལ' : 'ལམ', 'ས' : 'སམ', 'མཐའ་མེད' : 'འམ', 'ད་དྲག' : 'ཏམ'}\n",
    "    dagdra = {'ག' : 'པ', 'ད' : 'པ', 'བ' : 'པ', 'ས' : 'པ', 'ན' : 'པ', 'མ' : 'པ', 'ག' : 'པོ', 'ད' : 'པོ', 'བ' : 'པོ', 'ས' : 'པོ', 'ན' : 'པོ', 'མ' : 'པོ', 'ང' : 'བ', 'འ' : 'བ', 'ར' : 'བ', 'ལ' : 'བ', 'ང' : 'བོ', 'འ' : 'བོ', 'ར' : 'བོ', 'ལ' : 'བོ', 'མཐའ་མེད' : 'བ', 'མཐའ་མེད' : 'བོ', 'ད་དྲག' : 'པ', 'ད་དྲག' : 'པོ'}\n",
    "    lardu = {'ག' : 'གོ', 'ང' : 'ངོ', 'ད' : 'དོ', 'ན' : 'ནོ', 'བ' : 'བོ', 'མ' : 'མོ', 'འ' : 'འོ', 'ར' : 'རོ', 'ལ' : 'ལོ', 'ས' : 'སོ', 'མཐའ་མེད' : 'འོ', 'ད་དྲག' : 'ཏོ'}\n",
    "    cing = {'ག' : 'ཅིང', 'ད' : 'ཅིང', 'བ' : 'ཅིང', 'ང' : 'ཞིང', 'ན' : 'ཞིང', 'མ' : 'ཞིང', 'འ' : 'ཞིང', 'ར' : 'ཞིང', 'ལ' : 'ཞིང', 'ས' : 'ཤིང', 'མཐའ་མེད' : 'ཞིང', 'ད་དྲག' : 'ཅིང'}\n",
    "    ces = {'ག' : 'ཅེས', 'ད' : 'ཅེས', 'བ' : 'ཅེས', 'ང' : 'ཞེས', 'ན' : 'ཞེས', 'མ' : 'ཞེས', 'འ' : 'ཞེས', 'ར' : 'ཞེས', 'ལ' : 'ཞེས', 'ས' : 'ཞེས', 'མཐའ་མེད' : 'ཞེས', 'ད་དྲག' : 'ཅེས'}\n",
    "    ceo = {'ག' : 'ཅེའོ', 'ད' : 'ཅེའོ', 'བ' : 'ཅེའོ', 'ང' : 'ཞེའོ', 'ན' : 'ཞེའོ', 'མ' : 'ཞེའོ', 'འ' : 'ཞེའོ', 'ར' : 'ཞེའོ', 'ལ' : 'ཞེའོ', 'ས' : 'ཤེའོ', 'མཐའ་མེད' : 'ཞེའོ', 'ད་དྲག' : 'ཅེའོ', }\n",
    "    cena = {'ག' : 'ཅེ་ན', 'ད' : 'ཅེ་ན', 'བ' : 'ཅེ་ན', 'ང' : 'ཞེ་ན', 'ན' : 'ཞེ་ན', 'མ' : 'ཞེ་ན', 'འ' : 'ཞེ་ན', 'ར' : 'ཞེ་ན', 'ལ' : 'ཞེ་ན', 'ས' : 'ཤེ་ན', 'མཐའ་མེད' : 'ཞེ་ན', 'ད་དྲག' : 'ཅེ་ན'}\n",
    "    cig = {'ག' : 'ཅིག', 'ད' : 'ཅིག', 'བ' : 'ཅིག', 'ང' : 'ཞིག', 'ན' : 'ཞིག', 'མ' : 'ཞིག', 'འ' : 'ཞིག', 'ར' : 'ཞིག', 'ལ' : 'ཞིག', 'ས' : 'ཤིག', 'མཐའ་མེད' : 'ཞིག', 'ད་དྲག' : 'ཅིག', }\n",
    "    # mostly for modern spoken Tibetan. in accord with Esukhia’s decision to make the agreement for this \"new\" particle\n",
    "    gin = {'ད' : 'ཀྱིན', 'བ' : 'ཀྱིན', 'ས' : 'ཀྱིན', 'ག' : 'གིན', 'ང' : 'གིན', 'ན' : 'གྱིན', 'མ' : 'གྱིན', 'ར' : 'གྱིན', 'ལ' : 'གྱིན', 'ད་དྲག' : 'ཀྱིན'}\n",
    "    cases = [([\"གི\", \"ཀྱི\", \"གྱི\", \"ཡི\"], dreldra), ([\"གིས\", \"ཀྱིས\", \"གྱིས\", \"ཡིས\"], jedra), ([\"སུ\", \"ཏུ\", \"དུ\", \"རུ\"] , ladon), ([\"སྟེ\", \"ཏེ\", \"དེ\"], lhakce), ([\"ཀྱང\", \"ཡང\", \"འང\"], gyendu), ([\"གམ\", \"ངམ\", \"དམ\", \"ནམ\", \"བམ\", \"མམ\", \"འམ\", \"རམ\", \"ལམ\", \"སམ\", \"ཏམ\"], jedu), ([\"པ\", \"པོ\", \"བ\", \"བོ\"], dagdra), ([\"གོ\", \"ངོ\", \"དོ\", \"ནོ\", \"བོ\", \"མོ\", \"འོ\", \"རོ\", \"ལོ\", \"སོ\", \"ཏོ\"] , lardu), ([\"ཅིང\",  \"ཤིང\", \"ཞིང\"], cing), ([\"ཅེས\",  \"ཞེས\"], ces), ([\"ཅེའོ\",  \"ཤེའོ\",  \"ཞེའོ\"], ceo), ([\"ཅེ་ན\",  \"ཤེ་ན\",  \"ཞེ་ན\"], cena), ([\"ཅིག\",  \"ཤིག\",  \"ཞིག\"], cig), (['ཀྱིན', 'གིན', 'གྱིན'], gin)]\n",
    "    correction = ''\n",
    "    for case in cases:\n",
    "        if particle in case[0]:\n",
    "            correction = case[1][final]\n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pos(dict, key):\n",
    "    if key.endswith('་'):\n",
    "        key = key[:-1]\n",
    "    return dict.get(key)\n",
    "\n",
    "def get_main_pos(key, num):\n",
    "    if num == 0:\n",
    "        Dict = monlam_pos\n",
    "    if num == 1:\n",
    "        Dict = hill_pos\n",
    "    pos = get_pos(Dict, key)\n",
    "\n",
    "    main_pos = []\n",
    "    if pos != None:\n",
    "        # Monlam\n",
    "        if num == 0:\n",
    "            parts = pos.split('/')\n",
    "            for part in parts:\n",
    "                if part != '':\n",
    "                    main_pos.append(part.split(':')[0])\n",
    "\n",
    "        # Hill\n",
    "        if num == 1:\n",
    "            parts = pos.split('/')\n",
    "            for part in parts:\n",
    "                if part != '':\n",
    "                    if '.' in part:\n",
    "                        main_pos.append(part.split('.')[0])\n",
    "                    else:\n",
    "                        main_pos.append(part)        \n",
    "    return main_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['མིང་ཚིག', 'བྱ་ཚིག']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_main_pos('གཟུགས', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(List, entry):\n",
    "    global len_lexicon\n",
    "    index = bisect_left(List, entry, 0, len_lexicon)\n",
    "    return(True if index != len_lexicon and List[index] == entry else False)\n",
    "    \n",
    "def isWord(syls):\n",
    "    global total\n",
    "    maybe = '་'.join(syls)\n",
    "    final = False\n",
    "    if search(lexicon, maybe) == True:\n",
    "        final = True\n",
    "    elif search(lexicon, re.sub(merged_part, '', maybe)) == True:\n",
    "        final = True\n",
    "    return final\n",
    "\n",
    "def process(list1, list2, num):\n",
    "    global c\n",
    "    word = '་'.join(list1[c:c+num])\n",
    "    if search(lexicon, word) == False:\n",
    "        maybe = re.split(merged_part, word)\n",
    "        list2.append(maybe[0])\n",
    "        list2.append(maybe[1]+'་')\n",
    "        #del list1[:num]\n",
    "        c = c + num\n",
    "    else:\n",
    "        list2.append(word+\"་\")\n",
    "        #del list1[:num]\n",
    "        c = c + num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' །་']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"[།|༎|༏|༐|༑|༔|\\s]+་\", ' །་'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'རྒྱ་གར'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ཐེག', 'པའི', 'སྒོ', 'ཀུན', 'ལས', 'བཏུས', 'པ', 'གསུང', 'རབ', 'རིན', 'པོ', 'ཆེའི', 'མཛོད', 'བསླབ', 'པ', 'གསུམ', 'ལེགས', 'པར', 'སྟོན', 'པའི', 'བསྟན', 'བཅོས', 'ཤེས', 'བྱ', 'ཀུན', 'ཁྱབ', 'ཅེས', 'བྱ', 'བ', '། །\\xa0།', 'སྨྲ', 'བའི', 'སེང', 'གེས', 'དྲི', 'མེད', 'བློ', 'གྲོས', 'སྩོལ', '།\\xa0།', 'རིགས', 'ཁམས', 'ས', 'བོན', 'སྨིན', 'གྲོལ', 'གདམས', 'པས', 'གསོ', '།\\xa0།', 'རིམ', 'གཉིས', 'ལམ', 'གྱིས', 'གློ', 'བུར', 'དྲི', 'མ', 'སྦྱོང༌', '།\\xa0།', '']\n",
      "ལས\n",
      "པ\n",
      "རབ\n",
      "པ\n",
      "པར\n",
      "བ\n",
      "ཁམས\n",
      "ས\n",
      "གདམས\n",
      "པས\n",
      "ལམ\n",
      "མ\n"
     ]
    }
   ],
   "source": [
    "truc = 'ཐེག་པའི་སྒོ་ཀུན་ལས་བཏུས་པ་གསུང་རབ་རིན་པོ་ཆེའི་མཛོད་བསླབ་པ་གསུམ་ལེགས་པར་སྟོན་པའི་བསྟན་བཅོས་ཤེས་བྱ་ཀུན་ཁྱབ་ཅེས་བྱ་བ། ། །སྨྲ་བའི་སེང་གེས་དྲི་མེད་བློ་གྲོས་སྩོལ། །རིགས་ཁམས་ས་བོན་སྨིན་གྲོལ་གདམས་པས་གསོ། །རིམ་གཉིས་ལམ་གྱིས་གློ་བུར་དྲི་མ་སྦྱོང༌། །'\n",
    "syls = re.sub(r\"([།|༎|༏|༐|༑|༔|\\s]+)\", \"་\\g<1>་\", truc)\n",
    "syls = re.split(r'་+', syls)\n",
    "print(syls)\n",
    "for syl in syls:\n",
    "    if syl.isalpha():\n",
    "        print(syl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ཤེས་རབ་ཀྱི་ཕ་རོལ་ ཏུ་ དུ་\n",
      "ཤེས་རབ་ཀྱི་ཕ་རོལ་ ཏུ་ དུ་\n",
      "འདས་ཟབ་མོ་སྣང་བ་ ཞེས་ ཅེས་\n",
      "ཤེས་རབ་ཀྱི་ཕ་རོལ་ ཏུ་ དུ་\n",
      "པོ་དེ་དག་ལ་ ཀྱང་ ཡང་\n",
      "རིགས་ཀྱི་བུ འམ་ བམ་\n",
      "ཤེས་རབ་ཀྱི་ཕ་རོལ་ ཏུ་ དུ་\n",
      "རིགས་ཀྱི་བུ འམ་ བམ་\n",
      "ཤེས་རབ་ཀྱི་ཕ་རོལ་ ཏུ་ དུ་\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ྱ'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-615e576dcd09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnon_amb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mprevious\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpart_agreement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevious\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'་'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcorr\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-127-4d99f0f543fa>\u001b[0m in \u001b[0;36mpart_agreement\u001b[1;34m(previous, particle)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcases\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mcorrection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ྱ'"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('./IN/'):\n",
    "    if file.endswith(\".txt\"):\n",
    "    #todo - replace \\n by \\s try: with open('drugs') as temp_file: \\n drugs = [line.rstrip('\\n') for line in temp_file]\n",
    "        try:\n",
    "            with open('./IN/' + file, 'r', -1, 'utf-8-sig') as f:\n",
    "                current_file = f.read().replace('\\n', '').replace('\\r\\n', '')\n",
    "\n",
    "        except:\n",
    "            print(\"Save all IN files as UTF-8 and try again.\")\n",
    "            #input()\n",
    "    else:\n",
    "        print(\"\\nSave all IN files as text files and try again.\")\n",
    "        #input()\n",
    "    #start = time.time()\n",
    "    ######################\n",
    "    # Segmentation process\n",
    "    merged_part = r'(ར|ས|འི|འམ|འང)$'\n",
    "    \n",
    "    syls = re.sub(r\"([།|༎|༏|༐|༑|༔|\\s]+)\", \"་\\g<1>་\", current_file)\n",
    "    syls = re.split(r\"་+\", syls)\n",
    "    \n",
    "    non_words = []\n",
    "    words = []\n",
    "    c = 0\n",
    "    while c < len(syls):\n",
    "        if   isWord(syls[c:c+4]): process(syls, words, 4)\n",
    "        elif isWord(syls[c:c+3]): process(syls, words, 3)\n",
    "        elif isWord(syls[c:c+2]): process(syls, words, 2)\n",
    "        elif isWord(syls[c:c+1]): process(syls, words, 1)\n",
    "        else:\n",
    "            words.append('་'.join(syls[c:c+1])+\"་*\")\n",
    "            non_words.append('་'.join(syls[c:c+1])+\"་\")\n",
    "            c = c + 1\n",
    "    #\n",
    "    ######################\n",
    "    \n",
    "    #end = time.time()\n",
    "    ######################\n",
    "    # particle check\n",
    "    for num, word in enumerate(words):\n",
    "        \n",
    "        # all non-ambiguous particles :\n",
    "        #'གི', 'ཀྱི', 'གྱི', 'གིས', 'ཀྱིས', 'ཡིས', 'ཏུ', 'རུ', 'སྟེ', 'ཏེ', 'ཀྱང', 'ཡང', 'འང', \n",
    "        #'མམ', 'འམ', 'སམ', 'ཏམ', 'ནོ', 'ཏོ', \n",
    "        #'ཅིང', 'ཅེས', 'ཅེའོ', 'ཅིག', 'ཞེས', 'ཞེའོ', 'ཞིག', 'ཤིང', 'ཤེའོ', 'ཤིག'\n",
    "        non_amb = ['གི', 'ཀྱི', 'གྱི', 'གིས', 'ཀྱིས', 'ཡིས', 'ཏུ', 'རུ', 'སྟེ', 'ཏེ', 'ཀྱང', 'ཡང', 'འང', 'མམ', 'འམ', 'སམ', 'ཏམ', 'ནོ', 'ཏོ', 'ཅིང', 'ཅེས', 'ཅེའོ', 'ཅིག', 'ཞེས', 'ཞེའོ', 'ཞིག', 'ཤིང', 'ཤེའོ', 'ཤིག']\n",
    "        non_amb = non_amb + ['ངམ', 'མོ', 'འོ', 'རུ'] # non-amb for sherchin and teutshok\n",
    "        if word[:-1] in non_amb:\n",
    "            previous = words[num-1]\n",
    "            corr = part_agreement(previous[:-1], word[:-1])+'་'\n",
    "            if corr != word:\n",
    "                print(''.join(words[num-3:num]), word, corr)\n",
    "                words[num] = corr\n",
    "        \n",
    "        # དེ་ : if preceded by a noun, it is a pronoun, if preceded by a verb, it is a particle\n",
    "        if word == 'དེ་':\n",
    "            previous = words[num-1]\n",
    "            monlam_previous = get_main_pos(previous, 0)\n",
    "            monlam_previous = list(set(monlam_previous))\n",
    "            hill_previous = get_main_pos(previous, 1)\n",
    "            hill_previous = list(set(hill_previous))\n",
    "            if 'བྱ་ཚིག' in monlam_previous or 'v' in hill_previous:\n",
    "                words[num] = words[num]+'P'\n",
    "                corr = part_agreement(previous[:-1], word)\n",
    "                if corr != word:\n",
    "                    words[num] = words[num] + corr\n",
    "                words[num-1] = words[num-1]+'#'+'v'\n",
    "                \n",
    "        # hack to remove all * in the punctuation\n",
    "        if re.findall(r'[།|༎|༏|༐|༑|༔|༄|༅|\\s]+', word) != []:\n",
    "            words[num] = words[num].replace('*', '')\n",
    "    #['ཡི', 'གྱིས', 'སུ', 'དུ', 'ལམ', 'གམ', 'ངམ', 'དམ', 'ནམ', 'བམ', 'རམ', 'པ', 'པོ', 'བ', 'བོ', 'གོ', 'ངོ', 'དོ', 'མོ', 'འོ', 'རོ', 'ལོ', 'སོ', 'དེ',  'ཞིང']\n",
    "    #\n",
    "    ######################\n",
    "    #print(end-start)\n",
    "    ######################\n",
    "    # count percentage of POS tagged words\n",
    "    #pos_tagged = []\n",
    "    #for word in words:\n",
    "    #    wor = word\n",
    "    #    if word.endswith('་'):\n",
    "    #        wor = word[:-1]\n",
    "    #    if wor in monlam_pos.keys() and wor in hill_pos.keys():\n",
    "    #        pos_tagged.append(word+'|'+hill_pos[wor]+'#'+monlam_pos[wor])\n",
    "    #    elif wor in monlam_pos.keys() and wor not in hill_pos.keys():\n",
    "    #        pos_tagged.append(word+'|'+monlam_pos[wor])\n",
    "    #    elif wor not in monlam_pos.keys() and wor in hill_pos.keys():\n",
    "    #        pos_tagged.append(word+'|'+hill_pos[wor])\n",
    "    #    else:\n",
    "    #        pos_tagged.append(word)\n",
    "\n",
    "    #pos = 0\n",
    "    #no_pos = 0\n",
    "    #for word in pos_tagged:\n",
    "    #    if '|' in word:\n",
    "    #        pos = pos+1\n",
    "    #    else:\n",
    "    #        no_pos = pos+1\n",
    "    #print(pos, 'words out of', len(pos_tagged), 'have a POS.', '\\n', pos*100/len(pos_tagged), '% of the words have tags.')\n",
    "    #\n",
    "    ######################\n",
    "    \n",
    "    ######################\n",
    "    # flag particles\n",
    "    \n",
    "    \n",
    "    #\n",
    "    ######################\n",
    "    \n",
    "    ######################\n",
    "    # flag verbs\n",
    "    \n",
    "    \n",
    "    #\n",
    "    ######################\n",
    "    \n",
    "\n",
    "    # add linebreaks after 400 words\n",
    "    for i in range(w_nb-1, len(words), w_nb):\n",
    "        words[i] += '\\n'\n",
    "    \n",
    "    # write output\n",
    "    with open('./OUT/' + 'seg_' + file, 'w', -1, 'utf-8-sig') as f:\n",
    "        f.write(' '.join(words))\n",
    "    \n",
    "    with open('nonwords_' + file, 'w', -1, 'utf-8-sig') as f:\n",
    "        f.write('\\n'.join(sorted(list(set(non_words)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['༄༅་',\n",
       " '། །་',\n",
       " 'ཤེས་རབ་',\n",
       " 'ཀྱི་',\n",
       " 'ཕ་རོལ་',\n",
       " 'ཏུ་',\n",
       " 'ཕྱིན་པ',\n",
       " 'འི་',\n",
       " 'སྙིང་པོ་',\n",
       " 'ནི་',\n",
       " '།་',\n",
       " 'རྒྱ་གར་',\n",
       " 'སྐད་',\n",
       " 'དུ་',\n",
       " '། ་',\n",
       " 'བྷ་*',\n",
       " '༼༚༽ག་*',\n",
       " 'ཝ་ཏི་',\n",
       " 'པྲཛྙཱ་*',\n",
       " '༼༚༽པ་*']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
